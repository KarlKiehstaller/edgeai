
# Was ist Edge-AI?

![Cloud AI](./images/cloud.png){fig-align="left" width="40%" height="30%" style="margin-right: 200px; margin-left: 20px"}
![Cloud AI](./images/data.png){fig-align="left" width="40%" height="30%"}


# Projektziele

::: {.callout-tip}
## Visual-Odometry-System auf dem STM32N6 Edge-Prozessor
- Eingebettetes System
- Echtzeit-Bewegungsschätzung mit neuronalen Netz

:::


::: {.columns}
:::: {.column width="50%"}
**Bewertungskriterien**

- Genauigkeit des optimierten Neuronalen Netzes

- Frames per Second (FPS) bzw. Inferenzzeit

- Speicherverbrauch
::::
:::: {.column width="50%"}
![STM32N6](./images/stm32n6_nn.png){width="75%"}
::::
:::


# Edge-Device: STM32N6570-DK



- ARM Cortex-M55
- embedded NPU
- Camera Module
- 1 GBit external flash memory
- LCD display

![Eval Board](images/stm32n6.png)


## Neural Processing Unit

![Cloud AI](./images/npu.png){fig-align="center" width="40%" height="30%" style="margin-right: 50px; margin-left: 20px"}
![Cloud AI](./images/npuvscpuklein.png){fig-align="left" width="50%" height="30%" style="margin-top: 100px"}

<!-- Mehr Infos? -->

# Software

![](./images/softwarediagramm1.svg){fig-align="left" width="80%" }

<!-- Diagram unverständlich -->

## Getestete Modelle

::: {.columns}
:::: {.column width="60%"}
*Superpoint*

- Feature-Detektion und -Beschreibung anhand von Bildern

- **Eingang**: 480 × 640 Graustufenbild

- **Ausgang**: Keypointkandidaten + ID des Keypoints

::: {.callout-important .fragment}
### Positionsbestimmung muss zusätzlich implementiert werden
:::

::::

:::: {.column width="40%"}
![Keypoint-Detection](./images/keypoint.png){width="75%"}
::::

:::

::: {.columns .fragment}
:::: {.column width="60%"}
*Dronet*

- Steuerung eines Fahrzeuges anhand von Kamerabildern
- **Eingang**: 200x200 Graustufenbild
- **Ausgang**: Lenkwinkel und Kollisionswahrscheinlichkeit 

::::

:::: {.column width="40%"}
![Dronet](./images/Dronet.png){width="75%"}

::::
:::


## Quantisierung

:::{.columns}

:::: {.column width="50%"}
![Quantisierung eines Netzes](./images/quantisierung.png)

::::

:::: {.column width="50%"}
- Quantisierung wandelt Gleitkommazahlen (Float32) in Ganzzahlen (INT8) um, damit das Netz schneller und speichersparender wird.

::: {.fragment}
- Grober Ablauf:
    1. **Quantization**: Gewichte/Aktivierungen werden mithilfe von Skalierungsparametern (Scale/Zero-Point) gerundet.
    2. **Execution:** Die Inferenz wird epochenweise mit den gerundeten Werten berechnet.
    3. **Dequantization:** Die Ergebnisse werden mithilfe derselben Skalierungsparameter wieder in Gleitkommawerte zurückgerechnet.

:::

::::

:::

## Dronet V3 Quantisierung

![ST Edge AI Developer Cloud](./images/edge_ai_software.png)

### Dronet V3 Quantisierung {.unnumbered .unlisted}

<!-- Folie überarbeiten! -->
Die Quantisierungs-Pipeline des Dronet-V3 Netzes lief wie folgt ab:

```{mermaid fig-align="center"}
%%{init: {
  'theme': 'base', 'themeVariables': { 'fontSize': '24px'}}}%%
flowchart LR
  A["Vortrainiertes Modell <br/>in .tflite umwandeln"] --> 
  B["Ein-/Ausgabe-<br/>Architektur anpassen <br/>(inkompatible Ope- <br/>ratoren modifizieren)"] --> 
  C["Post-Training-<br/>Quantization (PTQ) <br/>mit LiteRT"] --> 
  D["Validierung &      <br/>Analyse mit      STM <br/> X-Cube-AI    "]

  %% Styling
  classDef redNode fill:#ff0350,color:#ffffff,stroke:#ff0350,stroke-width:2px,rx:30,ry:30;
  class A,B,C,D redNode;

  %% Graue Pfeile
  linkStyle default stroke:#b4b4b4,stroke-width:5px;
```

:::{.columns}

:::: {.column width="50%"}
![](./images/tensorflow-lite-logo-social.webp){width="40%" fig-align="center"}
::::
:::: {.column width="50%"}
![](./images/x_cube_ai.png){width="40%" fig-align="center"}
::::
:::


::: {.callout-important .fragment}

## Wichtig

Viele vortrainierte Netze wurden für GPU/CPU-Inferenz entworfen und verwenden Operatoren, die auf der NPU nicht nativ unterstützt werden. In diesen Fällen sind manuelle Anpassungen an der Modellarchitektur nötig, z. B. durch Ersetzen oder Umstrukturieren von Layern bzw. Operationen.
:::

# Implementierung

![](./images/ablauf.png){fig-align="center" width="80%"}

::: {.callout-important .fragment}
## Herausforderungen
- *Modellarchitektur* NPU-inkompatible Operationen → CPU-Fallback
- *Begrenzte Ressourcen* Wenig Online-Dokumentation für STM32N6
- *Hardware* Kamera/Display-Ansteuerung, Flashen, Speicherarchitektur
- *Software* Toolchain-Konfiguration, Datenpipeline-Stabilität
:::

# Evaluierung

![](./images/STM32N6_Inference.mp4){fig-align="center" width=70%}

# Evaluierung {.unnumbered .unlisted}

*Vergleich: Original vs Quantisiert*


| Metrik | Speicherbedarf | CPU | NPU | Inferenz | Steering | Collision |
|--------|----------------|-----|-----|----------|----------|-----------|
| **Original** | 1309 kB | 100% | 0% | 785.6 ms | 100% | 100% |
| **Quantisiert** | 707 kB | 8.3% | 91.7% | 1.648 ms | 98.1% | 97.0% |


::: {.callout-tip .fragment}
## Ergebnisse

- **geringer Speicherdarf**: 46% Reduktion 

- **kürzere Inferenzzeit**: 1.27fps → 607 fps 

- **hohe Genauigkeit**: Steering -1.9%, Collision -3.0%

- **geringere CPU Auslastung**

:::



# Ausblick

:::{.columns}

:::: {.column width="60%"}

*Einsatzgebiete*

1. **Autonome Fahrzeuge**: Echtzeit-Navigation ohne GPS

2. **Mobile Roboter**: Indoor-Navigation in GPS-freien Umgebungen

3. **Drohnen**: Visuelle Positionierung und Hindernisvermeidung

4. **Industrierobotik**: Präzise Bewegungssteuerung


::: {.fragment}

*Zukünftige Optimierungen*

1. Multi-Sensor-Fusion (IMU + Kamera)

2. Testen weiterer Modelle

:::

::: {.callout-note .fragment}
## Gibt es noch Fragen?
:::

::::

:::: {.column width="40%"}
![Autonomes Fahren](./images/Autonomes-Fahren.webp){width="75%"}

![Autonomer Industrieroboter](./images/intustrieroboter.webp){width="75%"}

::::
:::








