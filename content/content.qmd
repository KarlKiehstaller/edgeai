
# Was ist Edge-AI?

![Cloud AI](./images/cloud.png){fig-align="left" width="40%" height="30%" style="margin-right: 200px; margin-left: 20px"}
![Cloud AI](./images/data.png){fig-align="left" width="40%" height="30%"}


# Projektziele

::: {.callout-tip}
## Visual-Odometry-System auf dem STM32N6 Edge-Prozessor
- Eingebettetes System
- Echtzeit-Bewegungsschätzung mit neuronalen Netz

:::


::: {.columns}
:::: {.column width="50%"}
**Bewertungskriterien**

- Genauigkeit des optimierten Neuronalen Netzes

- Frames per Second (FPS) bzw. Inferenzzeit

- Speicherverbrauch
::::
:::: {.column width="50%"}
![STM32N6](./images/stm32n6_nn.png){width="75%"}
::::
:::


# Edge-Device: STM32N6570-DK



- ARM Cortex-M55
- embedded NPU
- Camera Module
- 1 GBit external flash memory
- LCD display

![Eval Board](images/stm32n6.png)


## Neural Processing Unit

![Cloud AI](./images/npu.png){fig-align="center" width="40%" height="30%" style="margin-right: 50px; margin-left: 20px"}
![Cloud AI](./images/npuvscpuklein.png){fig-align="left" width="50%" height="30%" style="margin-top: 100px"}

<!-- Mehr Infos? -->

# Software

![](./images/softwarediagramm1.svg){fig-align="left" width="80%" }

<!-- Diagram unverständlich -->

## Getestete Modelle

::: {.columns}
:::: {.column width="60%"}
*Superpoint*

- Feature-Detektion und -Beschreibung anhand von Bildern

- **Eingang**: 480 × 640 Graustufenbild

- **Ausgang**: Keypointkandidaten + ID des Keypoints

::: {.callout-important .fragment}
### Positionsbestimmung muss zusätzlich implementiert werden
:::

::::

:::: {.column width="40%"}
![Keypoint-Detection](./images/keypoint.png){width="75%"}
::::

:::

::: {.columns .fragment}
:::: {.column width="60%"}
*Dronet*

- Steuerung eines Fahrzeuges anhand von Kamerabildern
- **Eingang**: 200x200 Graustufenbild
- **Ausgang**: Lenkwinkel und Kollisionswahrscheinlichkeit 

::::

:::: {.column width="40%"}
![Dronet](./images/Dronet.png){width="75%"}

::::
:::


## Quantisierung

:::{.columns}

:::: {.column width="50%"}
![Quantisierung eines Netzes](./images/quantisierung.png)

::::

:::: {.column width="50%"}
- Quantisierung wandelt Gleitkommazahlen (Float32) in Ganzzahlen (INT8) um, damit das Netz schneller und speichersparender wird.

::: {.fragment}
- Grober Ablauf:
    1. **Quantization**: Gewichte/Aktivierungen werden mithilfe von Skalierungsparametern (Scale/Zero-Point) gerundet.
    2. **Execution:** Die Inferenz wird epochenweise mit den gerundeten Werten berechnet.
    3. **Dequantization:** Die Ergebnisse werden mithilfe derselben Skalierungsparameter wieder in Gleitkommawerte zurückgerechnet.

:::

::::

:::

### Dronet V3 Quantisierung

![ST Edge AI Developer Cloud](./images/edge_ai_software.png)

### Dronet V3 Quantisierung {.unnumbered .unlisted}

<!-- Folie überarbeiten! -->


:::{.columns}

:::: {.column width="50%"}
- Bei inkompatiblen Modellarchitekturen sind eigene Anpassungen nötig, in etwa durch das Ersetzen oder Umstrukturieren verschiedener Layer oder Operationen.

- Auch bei Dronet V3 war dies nötig.
::::

:::: {.column width="50%"}
**Umsetzung**:

- Umwandlung des vortrainierten Modells in das .tflite-Format.

- Manuelle Anpassung der Eingabe/Ausgabe-Architektur, da diese standardmäßig inkompatible Operatoren beinhalteten.

- Post-Training-Quantization (PTQ) des angepassten Modells mit der LiteRT-Bibliothek.

- Validierung und Analyse des quantisierten Modells mit der STM X-Cube-AI Toolchain.
::::

:::

# Implementierung

![](./images/ablauf.png){fig-align="center"}

::: {.callout-important .fragment}
## Herausforderungen
- *Modellarchitektur* NPU-inkompatible Operationen → CPU-Fallback
- *Begrenzte Ressourcen* Wenig Online-Dokumentation für STM32N6
- *Hardware* Kamera/Display-Ansteuerung, Flashen, Speicherarchitektur
- *Software* Toolchain-Konfiguration, Datenpipeline-Stabilität
:::

# Evaluierung

![](./images/STM32N6_Inference.mp4){fig-align="center" width=70%}

# Evaluierung {.unnumbered .unlisted}

*Vergleich: Original vs Quantisiert*


| Metrik | Speicherbedarf | CPU | NPU | Inferenz | Steering | Collision |
|--------|----------------|-----|-----|----------|----------|-----------|
| **Original** | 1309 kB | 100% | 0% | 785.6 ms | 100% | 100% |
| **Quantisiert** | 707 kB | 8.3% | 91.7% | 1.648 ms | 98.1% | 97.0% |


::: {.callout-tip .fragment}
## Ergebnisse

- **geringer Speicherdarf**: 46% Reduktion 

- **kürzere Inferenzzeit**: 1.27fps → 607 fps 

- **hohe Genauigkeit**: Steering -1.9%, Collision -3.0%

- **geringere CPU Auslastung**

:::



# Ausblick

:::{.columns}

:::: {.column width="60%"}

*Einsatzgebiete*

1. **Autonome Fahrzeuge**: Echtzeit-Navigation ohne GPS

2. **Mobile Roboter**: Indoor-Navigation in GPS-freien Umgebungen

3. **Drohnen**: Visuelle Positionierung und Hindernisvermeidung

4. **Industrierobotik**: Präzise Bewegungssteuerung


::: {.fragment}

*Zukünftige Optimierungen*

1. Multi-Sensor-Fusion (IMU + Kamera)

2. Testen weiterer Modelle

:::

::: {.callout-note .fragment}
## Gibt es noch Fragen?
:::

::::

:::: {.column width="40%"}
![Autonomes Fahren](./images/Autonomes-Fahren.webp){width="75%"}

![Autonomer Industrieroboter](./images/intustrieroboter.webp){width="75%"}

::::
:::








